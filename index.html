<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Introducing Apache Spark</title>

		<meta name="description" content="Introducing fundamental Apache Spark concepts through diagrams, examples and use cases.">
		<meta name="author" content="Sean McIntyre">

		<meta name="apple-mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/black.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Custom styles -->
		<link rel="stylesheet" href="css/custom.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section data-transition="convex">
					<h1>Intro to<br/>
							Apache Spark</h1>
					<p>Distributed, in-memory computing for data processing</p>
				</section>

				<!--BEGIN: What is it?-->
				<section>
					<section>
						<h2>What is it?</h2>
					</section>
					<section>
						<ul>
							<li>
								General-purpose cluster computing* framework and runtime
								<br/><small class="fragment"><em>A <strong>framework</strong> is a library. A <strong>runtime</strong> is a thing that runs your code.</em></small>
								<br/><small class="fragment"><em>More on this in a second.</em></small>
							</li>
							<li class="fragment">Supports multiple languages (Scala, Python, Java, R)</li>
							<li class="fragment">Smart enough to do <em>some</em> optimization of your queries/transformations</li>
							<li class="fragment">Has higher-level libraries which are more domain-specific (Spark SQL, Spark Streaming, MLlib, GraphX)</li>
						</ul>
					</section>
					<section>
						<h3>Cluster Computing?</h3>
						<ul>
							<li class="fragment">Cluster computing is a broad term for dividing work between multiple computers</li>
							<li class="fragment">Each <strong>worker</strong> machine finishes a piece of a problem, then has their partial results combined on a <strong>master</strong> machine</li>
							<li class="fragment">Spark is <em>"general-purpose"</em>. This <strong>doesn't mean it does everything well</strong>. It means it isn't dedicated to one domain or use case.</li>
						</ul>
					</section>
				</section>
				<!--END: What is it?-->

				<!--BEGIN: How does it work?-->
				<section>
					<section>
						<h2>How does it work?</h2>
						<p class="fragment">
							Data -> Distributed Data -> Distributed Execution -> Collect
						</p>
					</section>
					<section>
						<h3>Step 1: Data</h3>
						<p class="fragment">Spark supports retrieving data from a variety of sources:</p>
						<ul class="fragment">
							<li>Filesystem</li>
							<li>HDFS</li>
							<li>Cassandra</li>
							<li>Amazon S3</li>
							<li>Relational Databases (JDBC)</li>
							<li>etc...</li>
						</ul>
					</section>
					<section>
						<h3>Step 2: Distributed Data</h3>
						<p class="fragment">With the exception of HDFS, these data sources do not represented <strong>distributed</strong> data. Dividing data between <strong>workers</strong> is one of the most important things Spark does.</p>
					</section>
					<section>
						<p>Let's look at how Spark deals with a relational database</p>
						<img src="images/how-it-works/data.png"/>
					</section>
					<section>
						<p>Spark starts by looking at the maximum and minimum primary key and divides that key space into <strong>partitions</strong>.</p>
						<img src="images/how-it-works/partitions.png"/>
					</section>
					<section>
						<p>Spark then divides the partitions among its worker nodes.</p>
						<img src="images/how-it-works/workers.png"/>
						<p class="fragment">
							This is a <em>bit</em> of a simplification, since workers can have multiple executor threads. But you get the idea.
						</p>
					</section>
					<section>
						<p>This process isn't magic!</p>
						<img src="images/how-it-works/skew.png"/>
						<p>Spark will not account for gaps in your primary key values, so the partitions might not actually be equally sized!<br/>This is called <strong>skew</strong> and can lead to some workers doing more work than others (bad).</p>
					</section>
					<section>
						<h3>Step 3 &amp; 4: Distributed Execution and Results Collection</h3>
						<p class="fragment">
							Spark executes <strong>parallel copies</strong> of your code against each data partition...
						</p>
						<ul>
							<li class="fragment">Shuffling data between worker nodes if it needs to</li>
							<li class="fragment">Sending results back to the master node at the end of your program (ideally)</li>
							<li class="fragment">We'll dive into this more when we start writing code</li>
						</ul>
					</section>
					<section>
						<p>
							This mapping between source data and partitions is handled by an <code>RDD</code>. <code>RDDs</code> are a low-level Spark concept.
						</p>
						<p>
							Most people use <code>Dataframes</code> and <code>Datasets</code> instead, which are more <strong>expressive</strong> and <strong>faster*</strong>
						</p>
						<small><em>*more on this later</em></small>
					</section>
				</section>
				<!--END: How does it work?-->

				<!--BEGIN: When does it suck?-->
				<section>
					<section>
						<h2>When should I use Spark?</h2>
						<p class="fragment">(and when is it the worst?)</p>
					</section>
					<section>
						<h3>Myth #1</h3>
						<h4>Spark is <strong>faster</strong> than an RDBMS or other database</h4>
						<p class="fragment">
							False!<br/>Spark really comes into play when you have so much data that you can't index it.
						</p>
						<p class="fragment">
							If you can, an index will almost* always be faster.
						</p>
						<small class="fragment">
							*Compute-heavy queries may run faster on Spark, but it's still a toss-up given the lack of an index.
						</small>
					</section>
					<section>
						<h3>Myth #2</h3>
						<h4>You can do <strong>anything</strong> with Spark</h4>
						<p class="fragment">
							Spark really only works well when the algorithm you're writing can easily be distributed.
						</p>
						<p class="fragment">
							 Problems that don't break down into bits which can be solved in parallel are poorly suited for Spark. Code like this will run <strong>slower</strong> on Spark than it would on a single machine!
						</p>
					</section>
					<section>
						<h3>Myth #3</h3>
						<h4>Spark Scala, Java, Python and R are equally fast</h4>
						<p class="fragment">
							This is far less true than the Spark docs claim.
						</p>
						<p class="fragment">
							I'll explain why later this evening...
						</p>
					</section>
				</section>
				<!--END: When does it suck?-->

				<!-- BEGIN spark fundamental tools -->
				<section>
					<section>
						<h2>Let's make something!</h2>
					</section>

					<section>
						<h3>Sample data</h3>
						<p>
							Here's one day of data from the popular <a href="http://www.andresmh.com/nyctaxitrips/">NYC Taxi Dataset</a>
						</p>
						<pre class="presentation-code">
							<code class="bash" data-trim>
$ curl -O http://assets.oculusinfo.com/salt/sample-data/taxi_one_day.csv
							</code>
						</pre>
						<span>
							Or click <a href="http://assets.oculusinfo.com/salt/sample-data/taxi_one_day.csv">here</a>
						</span>
						<p class="fragment">
							Either way, remember where you put it.<br/>
							We'll need the path to that file later.
						</p>
					</section>

					<section>
						<h3>spark-shell</h3>
						<p class="fragment">
							Now, let's fire up a spark-shell so we have somewhere convenient to experiment
						</p>
						<pre class="presentation-code fragment">
							<code class="sh" data-trim>
$ cd spark-1.6.1-bin-hadoop2.6/bin
$ ./spark-shell \
  --master local[2] \
  --packages com.databricks:spark-csv_2.10:1.4.0
							</code>
						</pre>
					</section>

					<section>
						<h3>Breakdown:</h3>
						<div class="fragment">
							<span>Give us two threads for executors:</span>
							<pre class="presentation-code">
								<code class="sh" data-trim>
--master local[2]
								</code>
							</pre>
						</div>
						<div class="fragment">
							<span>Download packages from maven central and put them on the classpath:</span>
							<pre class="presentation-code">
								<code class="sh" data-trim>
--packages com.databricks:spark-csv_2.10:1.4.0
								</code>
							</pre>
						</div>
					</section>

					<section>
						<h3>Useful Package</h3>
						<div class="fragment">
							<strong>spark-csv</strong> supports I/O for CSV/TSV/DSV files
							<pre class="presentation-code">
								<code class="gradle" data-trim>com.databricks:spark-csv_2.10:1.4.0</code>
							</pre>
						</div>
					</section>

					<section>
						<p>
							At this point, you should have a spark prompt:
						</p>
						<pre class="presentation-code">
...
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.6.1
      /_/

Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_77)
Type in expressions to have them evaluated.
Type :help for more information.
Spark context available as sc.
16/03/28 18:28:21 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
16/03/28 18:28:21 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
16/03/28 18:28:28 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
16/03/28 18:28:29 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
16/03/28 18:28:32 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
16/03/28 18:28:32 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
SQL context available as sqlContext.

scala>
						</pre>
					</section>
				</section>
				<!-- END spark fundamental tools -->

				<!-- BEGIN data ingestion -->
				<section>
					<section>
						<h2>Data Ingestion</h2>
					</section>
					<section>
						<h3>Definition:</h3>
						<p>
							A <strong>Dataframe</strong> is a wrapper around an <code>RDD</code> which:
						</p>
						<ul>
							<li class="fragment">Represents distributed data as columnar, with schemas and well-defined data types</li>
							<li class="fragment">Offers a SQL-like API for interacting with that data</li>
						</ul>
					</section>
					<section>
						<p>
							Let's create a <code><strong>Dataframe</strong></code> from taxi_one_day.csv:
						</p>
						<pre class="presentation-code">
							<code class="scala" data-trim>
// enter paste mode for multi-line stuff
scala> :paste
// modify the path and paste this code into the shell
val df = sqlContext.read
  .format("com.databricks.spark.csv")
  .option("header", "true") // Use first line of all files as header
  .option("inferSchema", "true") // Automatically infer data types
  .load("path/to/taxi_one_day.csv") // can also use a directory/*.csv here
// type the EOF character (CTRL+D) to exit paste mode and interpret
							</code>
						</pre>
					</section>
					<section>
						<p>
							Check out the inferred schema:
						</p>
						<pre class="presentation-code">
							<code class="scala" data-trim>
scala> df.printSchema
root
 |-- hack: string (nullable = true)
 |-- license: string (nullable = true)
 |-- code: string (nullable = true)
 |-- flag: integer (nullable = true)
 |-- type: string (nullable = true)
 |-- pickup_time: timestamp (nullable = true)
 |-- dropoff_time: timestamp (nullable = true)
 |-- passengers: integer (nullable = true)
 |-- duration: integer (nullable = true)
 |-- distance: double (nullable = true)
 |-- pickup_lon: double (nullable = true)
 |-- pickup_lat: double (nullable = true)
 |-- dropoff_lon: string (nullable = true)
 |-- dropoff_lat: string (nullable = true)
							</code>
						</pre>
					</section>
					<section>
						<p>
							<code>spark-csv</code> may not infer the column datatypes correctly. Notice that <code>pickup_lat</code> and <code>pickup_lon</code> are <code>doubles</code>, but their <code>dropoff</code> counterparts are <code>strings</code>.
						</p>
						<p class="fragment">
							This is likely due to the presence of a non-numeric character somewhere in the dataset.
						</p>
					</section>
				</section>
				<!-- END data ingestion -->

				<!-- BEGIN data querying -->
				<section>
					<section>
						<h3>Fundamental: Spark SQL</h3>
						<p class="fragment">
							<code><strong>Dataframe</strong></code>s are part of Spark SQL, and have been the primary mechanism for interacting with data in Spark for several versions.
						</p>
						<p class="fragment">
							Spark 1.6 introduces a preview of <code><strong>Dataset</strong></code>s, which attempt to combine the benefits of Dataframes and directly using RDDs. We won't cover them today, since they'll be released in full in Spark 2.0 this summer.
						</p>
					</section>
					<section>
						<p>
							Code written using <code>Dataframe</code>s is <strong>faster</strong> than code which uses <code>RDD</code>s!.
						</p>
						<p>
							This is because Spark can greatly optimize queries against columnar data, and is capable of doing a lot of the work using basic (unboxed) types rather than boxed types.
						</p>
					</section>
					<section>
						<h2>Querying</h2>
						<p>
							Let's try some simple queries against our data!
						</p>
					</section>
					<section>
						<pre class="presentation-code">
							<code class="scala" data-trim>
scala> df.first
scala> df.show
scala> df.select($"duration",$"distance").where($"distance" > 15)
							</code>
						</pre>
						<p class="fragment">
							Notice the last one returns a Dataframe instead of a result! This is because everything in Spark is <strong>lazily evaluated</strong>.
						</p>
						<p class="fragment">
							Calling <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$">transformations</a> like <code>select()</code> or <code>where()</code> always results in a new Dataframe with an optimized query.
						</p>
						<p class="fragment">
							Nothing gets returned until you actually <code>execute</code> the query by asking for a result via <a href="http://spark.apache.org/docs/latest/programming-guide.html#actions">actions</a> such as <code>first()</code>, <code>take()</code> or <code>collect().</code>
						</p>
					</section>
					<section>
						<h3>Common Actions</h3>
						<pre class="presentation-code">
							<code class="scala" data-trim>
scala> df.select($"duration",$"distance").where($"distance" > 15).first
scala> df.select($"duration",$"distance").where($"distance" > 15).take(5)
// you can also use plain strings when you don't need the $ "column syntax"
scala> df.select("duration","distance").where("distance > 15").show
scala> df.select("duration","distance").where("distance > 15").collect
scala> df.select("duration","distance").where("distance > 15").count
							</code>
						</pre>
					</section>
					<section>
						<h3>Common Transformations</h3>
						<pre class="presentation-code">
							<code class="scala" data-trim>
scala> df.select("duration","distance").show
// fix datatype on dropoff_lon
scala> df.selectExpr("cast(dropoff_lon as double) as longitude").show
scala> df.filter($"distance" > 15).show // same as where()
scala> df.limit(100).show
scala> df.groupBy("hack").agg(max("distance")).orderBy($"max(distance)".desc).show
							</code>
						</pre>
						<p class="fragment">
							Side note: this works the same way on parquet, json, etc. Nested objects can be accessed via dot notation: <code>df.select("person.name.first")</code>
						</p>
					</section>
					<section>
						<h3>Go nuts!</h3>
						<div class="fragment">
							<p>
								Try answering the following questions:
							</p>
							<ul>
								<li>Average location of pickups?</li>
								<li>Average pickup time?</li>
								<li>Taxi (hack) which carried the most passengers that day?</li>
								<li>Maximum and minimum dropoff longitude? What might this mean for our data quality?</li>
								<li>Maximum and minimum (sane) dropoff longitude?</li>
							</ul>
						</div>
						<aside class="notes">
							Rough NYC bounding box:
							lat: 40.560093 40.935434
							lon: -74.164634 -73.734040
						</aside>
					</section>
				</section>
				<!-- END data querying -->

				<!-- BEGIN intermediate spark -->
				<section>
					<section>
						<h2>Intermediate Spark</h2>
					</section>
					<section>
						<p>Since every Dataframe transformation returns a new Dataframe, you can always store it in a variable and query it several times:</p>
						<pre class="presentation-code">
							<code class="scala" data-trim>
scala> :paste
val df2 = df.select("hack", "duration", "distance")
            .where("distance > 15")
scala> :paste
df2.groupBy("hack")
   .agg(max("distance"))
	 .orderBy($"max(distance)".desc)
	 .show
							</code>
						</pre>
					</section>
					<section>
						<h3>Caching</h3>
						<p class="fragment">
							Every time you use an <strong>action</strong>, Spark goes right back to the source data. This is <strong>slow</strong>!
						</p>
						<p class="fragment">
							We can address this by using the <strong>cache()</strong> transformation on our Dataframe:
						</p>
						<pre class="presentation-code fragment">
							<code class="scala" data-trim>
scala> val cachedDf = df.cache
							</code>
						</pre>
						<p class="fragment">
							This is a hint to Spark that it should start keeping records in memory for future use.
						</p>
					</section>
					<section>
						<h3>A brief note about RDDs</h3>
						<div class="fragment">
							Dataframes offer several methods such as <code>map()</code>, <code>flatMap()</code>, <code>foreach()</code>, and others which:
							<ul>
								<li>Operate directly on the underlying RDD</li>
								<li>Return an RDD</li>
							</ul>
						</div>
						<p class="fragment">
							Always use these methods <strong>deliberately</strong> and <strong>conservatively</strong>, as there are potential performance penalties.
							<small><em>And RDDs have a completely different API :(</em></small>
						</p>
					</section>
					<section>
						Datasets in Spark 2.0 should make this a lot less confusing
					</section>
				</section>
				<!-- END intermediate spark -->

				<!-- BEGIN export -->
				<section>
					<section>
						<h2>Data Export</h2>
					</section>
					<section>
						<p>
							Let's talk a little bit about formats!
						</p>
					</section>
					<section>
						<h3>Spark Data Formats</h3>
						<p class="fragment">
							Spark natively supports the following export (and import) formats:
						</p>
						<ul class="fragment">
							<li>JSON (slow)</li>
							<li>Text</li>
							<li>Parquet (fastest for queries)</li>
							<li>ORC (small - mostly a Hive thing)</li>
							<li>JDBC</li>
						</ul>
						<p class="fragment">It's generally recommended to use Parquet - it's the default.</p>
					</section>
					<section>
						<pre class="presentation-code">
							<code class="scala" data-trim>
df.filter("distance > 40")
	.write
  .format("parquet")
  .save("output/taxi_parquet")
							</code>
						</pre>
					</section>
				</section>
				<!-- END export -->

				<!-- BEGIN demos -->
				<section>
					<section>
						<h2>What is Uncharted doing with Spark?</h2>
					</section>
					<section>
						<h3>Sparkpipe</h3>
					</section>
					<section>
						<h3>Salt</h3>
					</section>
				</section>
				<!-- END demos -->
			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			// Full list of configuration options available at:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				transition: 'slide', // none/fade/slide/convex/concave/zoom

				// Optional reveal.js plugins
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true }
				]
			});

		</script>

	</body>
</html>
